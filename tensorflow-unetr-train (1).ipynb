{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # UNETR: Transformers for 3D Medical Image Segmentation\n","metadata":{}},{"cell_type":"markdown","source":"\n# Methodlogy \n* In this notebook **2.5D** images are used for Training for **Segmentation** with `tf.data`, `tfrecord` using `Tensorflow`.  \n* In a nutshell, **2.5D Image Training** is training of **3D** image like **2D** Image. 2.5D images can take leverage of the extra depth information like our typical RGB image. 2.5D Images are built from 3 channels with 2 strides \n* The UNETR model from **[UNETR: Transformers for 3D Medical Image Segmentation](https://arxiv.org/pdf/2103.10504.pdf)** is used here (from the transunet library).\n* The model has 100M parameters that we need to train. To use TPU capabilities, the dataset has to be transformed into a TFRecord. I used the 2.5D image dataset created in this notebook by awsaf49: [UWMGI: 2.5D TFRecord Data](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-tfrecord-data).\n* \"TFRecord files are created using **StratifiedGroupFold** to avoid data leakage due to `case` and to stratify `empty` and `non-empty` mask cases\".\n* This notebook is compatible for both **GPU** and **TPU**. Device is automatically selected so you won't have to do anything to allocate device.\n* As there are overlaps between **Stomach**, **Large Bowel** & **Small Bowel** classes, this is a **MultiLabel Segmentation** task, so final activaion should be `sigmoid` instead of `softmax`.\n* You can play with different models and losses.\n","metadata":{}},{"cell_type":"markdown","source":"# Reference Notebooks and Datasets \n\n**UNETR Model**:\n* [UNETR](https://www.kaggle.com/code/usharengaraju/tensorflow-unetr-w-b)\n\n**2.5D-TransUNet**:\n* Train: [UWMGI: TransUnet 2.5D [Train] [TF]](https://www.kaggle.com/awsaf49/uwmgi-transunet-2-5d-train-tf/)\n<!-- * Infer:  UWMGI: TransUnet 2.5D [Infer] [TF]-->\n\n**Data/Dataset**:\n* Dataset: [UWMGI: 2.5D TFRecord Dataset](https://www.kaggle.com/datasets/awsaf49/uwmgi-25d-tfrecord-dataset)","metadata":{}},{"cell_type":"code","source":"!pip install  segmentation_models","metadata":{"execution":{"iopub.status.busy":"2022-07-10T15:52:47.749690Z","iopub.execute_input":"2022-07-10T15:52:47.750396Z","iopub.status.idle":"2022-07-10T15:52:58.678954Z","shell.execute_reply.started":"2022-07-10T15:52:47.750261Z","shell.execute_reply":"2022-07-10T15:52:58.677698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd, numpy as np, random,os, shutil\nimport tensorflow as tf\nimport re\nimport math\nimport cv2\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom kaggle_datasets import KaggleDatasets\n\nfrom PIL import Image\nfrom tqdm import tqdm\nimport glob\n\nimport math\nimport tensorflow as tf\nimport tensorflow.keras.backend as k\n\nimport os\nfrom typing import List, Tuple\nfrom pathlib import Path\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom skimage import io\nfrom skimage.color import gray2rgb\nfrom skimage.transform import resize\n#from rich.jupyter import print\n\n# Set tf.keras as backend\nos.environ['SM_FRAMEWORK'] = 'tf.keras'\nimport segmentation_models as sm\n","metadata":{"id":"oY8_qJ3KdH4Z","executionInfo":{"status":"ok","timestamp":1655598847076,"user_tz":-330,"elapsed":2352,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-07-10T15:52:58.682240Z","iopub.execute_input":"2022-07-10T15:52:58.682669Z","iopub.status.idle":"2022-07-10T15:53:08.953370Z","shell.execute_reply.started":"2022-07-10T15:52:58.682613Z","shell.execute_reply":"2022-07-10T15:53:08.952129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Data Preprocessing</span>**","metadata":{}},{"cell_type":"code","source":"# Code copied from https://www.kaggle.com/code/ayuraj/quick-data-eda-segmentation-viz-using-w-b\n\nROOT_DIR = '../input/uw-madison-gi-tract-image-segmentation/'\ndf = pd.read_csv(ROOT_DIR+'train.csv')\n# Remove rows with NaN Segmentation masks\ndf = df[df.segmentation.notna()].reset_index(drop=False)\ndef get_case_str(row):\n    case_num = row.id.split('_')[0]\n    return case_num\n\ndef get_case_id(row):\n    case_num = row.id.split('_')[0]\n    return int(case_num[4:])\n\ndf['case_str'] = df.apply(lambda row: get_case_str(row), axis=1)\ndf['case_id'] = df.apply(lambda row: get_case_id(row), axis=1)\n\ndef get_day_str(row):\n    return row.id.split('_')[1]\n\ndef get_day_id(row):\n    return int(row.id.split('_')[1][3:])\n\ndf['day_str'] = df.apply(lambda row: get_day_str(row), axis=1)\ndf['day_id'] = df.apply(lambda row: get_day_id(row), axis=1)\n\ndef get_slice_str(row):\n    slice_id = row.id.split('_')[-1]\n    return f'slice_{slice_id}'\n\ndf['slice_str'] = df.apply(lambda row: get_slice_str(row), axis=1)\nfilepaths = glob.glob(ROOT_DIR+'train/*/*/*/*')\n\n\nfile_df = pd.DataFrame(columns=['case_str', 'day_str', 'slice_str', 'filename', 'filepath'])\nfor idx, filepath in tqdm(enumerate(filepaths)):\n    case_day_str = filepath.split('/')[5]\n    case_str, day_str = case_day_str.split('_')\n\n    filename = filepath.split('/')[-1]\n    slice_id = filename.split('_')[1]\n    slice_str = f'slice_{slice_id}'\n    \n    file_df.loc[idx] = [case_str, day_str, slice_str, filename, filepath]\n\ndf = pd.merge(df, file_df, on=['case_str', 'day_str', 'slice_str'])\n\ndef get_image_height(row):\n    return int(row.filename[:-4].split('_')[2])\n    \ndef get_image_width(row):\n    return int(row.filename[:-4].split('_')[3])\n\ndef get_pixel_height(row):\n    return float(row.filename[:-4].split('_')[4])\n\ndef get_pixel_width(row):\n    return float(row.filename[:-4].split('_')[5])\n\ndf['img_height'] = df.apply(lambda row: get_image_height(row), axis=1)\ndf['img_width'] = df.apply(lambda row: get_image_width(row), axis=1)\ndf['pixel_height (mm)'] = df.apply(lambda row: get_pixel_height(row), axis=1)\ndf['pixel_width (mm)'] = df.apply(lambda row: get_pixel_width(row), axis=1)\n\ndf.drop('index', axis=1, inplace=True)\n\nby_case = df.groupby('case_str')\ncase_df = by_case.get_group('case123')\n\nby_day = case_df.groupby('day_str')\nday_df = by_day.get_group('day0')\n\nby_slice = day_df.groupby('slice_str')\nslice_df = by_slice.get_group('slice_0075')\n\n# saving the dataframe\ndf.to_csv('df.csv')\n# saving the dataframe\nslice_df.to_csv('slice_df.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-10T15:53:08.954930Z","iopub.execute_input":"2022-07-10T15:53:08.955380Z","iopub.status.idle":"2022-07-10T15:56:58.351127Z","shell.execute_reply.started":"2022-07-10T15:53:08.955321Z","shell.execute_reply":"2022-07-10T15:56:58.350340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n\n    debug = False   \n   \n    verbose = 0\n    display_plot = True\n\n    # Device for training\n    device = None  # device is automatically selected\n\n    # Seeding for reproducibility\n    seed = 101\n\n    # Image Size\n    img_size = [96, 96]\n\n    # Batch Size & Epochs\n    batch_size = 2\n    drop_remainder = False\n    epochs = 15\n    steps_per_execution = None\n\n    # Model & Backbone\n    model_name = \"UNETR\"\n    backbone = None\n    \n    # Loss & Optimizer & LR Scheduler\n    loss = \"dice_loss\"\n    optimizer = \"Adam\"\n    lr = 5e-4\n    lr_schedule = \"CosineDecay\"\n    patience = 5\n   \n    # Clip values to [0, 1]\n    clip = False\n    \n    # Number of folds\n    folds = 5\n\n    # Which Folds to train\n    selected_folds = [0, 1, 2, 3, 4]\n    \n    # Augmentation\n    augment = False\n    transform = False\n\nimport re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n","metadata":{"id":"KnjDZfbpGGBe","executionInfo":{"status":"ok","timestamp":1655598847076,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-07-10T16:00:04.714011Z","iopub.execute_input":"2022-07-10T16:00:04.714441Z","iopub.status.idle":"2022-07-10T16:00:04.723065Z","shell.execute_reply.started":"2022-07-10T16:00:04.714410Z","shell.execute_reply":"2022-07-10T16:00:04.722036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Set up device</span>**","metadata":{}},{"cell_type":"code","source":"try: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # for CPU/GPU or multi-GPU machines\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-10T15:56:58.364523Z","iopub.execute_input":"2022-07-10T15:56:58.364828Z","iopub.status.idle":"2022-07-10T15:57:04.499886Z","shell.execute_reply.started":"2022-07-10T15:56:58.364791Z","shell.execute_reply":"2022-07-10T15:57:04.499233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#strategy, CFG.device, tpu = configure_device()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2022-07-10T15:57:04.501460Z","iopub.execute_input":"2022-07-10T15:57:04.501737Z","iopub.status.idle":"2022-07-10T15:57:04.507572Z","shell.execute_reply.started":"2022-07-10T15:57:04.501706Z","shell.execute_reply":"2022-07-10T15:57:04.506724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/uw-madison-gi-tract-image-segmentation'\nGCS_PATH = KaggleDatasets().get_gcs_path('uwmgi-25d-tfrecord-dataset')\nALL_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/uwmgi/*.tfrec')\nprint('NUM TFRECORD FILES: {:,}'.format(len(ALL_FILENAMES)))\nprint('NUM TRAINING IMAGES: {:,}'.format(count_data_items(ALL_FILENAMES)))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-07-10T15:57:04.509072Z","iopub.execute_input":"2022-07-10T15:57:04.509320Z","iopub.status.idle":"2022-07-10T15:57:05.001526Z","shell.execute_reply.started":"2022-07-10T15:57:04.509291Z","shell.execute_reply":"2022-07-10T15:57:05.000659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Input Data Pipeline</span>**","metadata":{}},{"cell_type":"code","source":"# Decode image from bytestring to tensor\ndef decode_image(data, height, width, target_size=CFG.img_size):\n    img = tf.io.decode_raw(data, out_type=tf.uint16)\n    img = tf.reshape(img, [height, width, 3])  # explicit size needed for TPU\n    img = tf.cast(img, tf.float32)\n    img = tf.math.divide_no_nan(img, tf.math.reduce_max(img))  # scale image to [0, 1]\n    img = tf.image.resize_with_pad(\n        img, target_size[0], target_size[1], method=\"nearest\"\n    )  # resize with pad to avoid distortion\n    img = tf.reshape(img, [*target_size, 3])  # reshape after resize\n    return img\n\n\n# Decode mask from bytestring to tensor\ndef decode_mask(data, height, width, target_size=CFG.img_size):\n    msk = tf.io.decode_raw(data, out_type=tf.uint8)\n    msk = tf.reshape(msk, [height, width, 3])  # explicit size needed for TPU\n    msk = tf.cast(msk, tf.float32)\n    msk = msk / 255.0  # scale mask data to[0, 1]\n    msk = tf.image.resize_with_pad(\n        msk, target_size[0], target_size[1], method=\"nearest\"\n    )\n    msk = tf.reshape(msk, [*target_size, 3])  # reshape after resize\n    return msk\n\n\n# Read tfrecord data & parse it & do augmentation\ndef read_tfrecord(example, augment=True, return_id=False, dim=CFG.img_size):\n    tfrec_format = {\n        \"id\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n        \"height\": tf.io.FixedLenFeature([], tf.int64),\n        \"width\": tf.io.FixedLenFeature([], tf.int64),\n        \"mask\": tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(\n        example, tfrec_format\n    )  # parses a single example proto.\n    image_id = example[\"id\"]\n    height = example[\"height\"]\n    width = example[\"width\"]\n    img = decode_image(example[\"image\"], height, width, dim)  # access image\n    msk = decode_mask(example[\"mask\"], height, width, dim)  # access mask\n    img = tf.reshape(img, [*dim, 3])\n    msk = tf.reshape(msk, [*dim, 3])\n    img = tf.repeat(img[:, :, np.newaxis,:], 96, axis=2)\n    msk = tf.repeat(msk[:, :, np.newaxis,:], 96, axis=2)\n    return (img, msk) if not return_id else (img, image_id, msk)\n\ndef get_dataset(\n    filenames,\n    shuffle=False,\n    repeat=False,\n    augment=False,\n    cache=False,\n    return_id=False,\n    batch_size=CFG.batch_size ,\n    target_size=CFG.img_size,\n    drop_remainder=False,\n    seed=CFG.seed,\n):\n    dataset = tf.data.TFRecordDataset(filenames)  # read tfrecord files\n    dataset = dataset.map(\n        lambda x: read_tfrecord(\n            x,\n            augment=augment,  # unparse tfrecord data with masks\n            return_id=return_id,\n            dim=target_size,\n        ))\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)  # batch the data\n    dataset = dataset.prefetch(AUTO)  # prefetch data for speedup\n    return dataset\n\nds = get_dataset(ALL_FILENAMES, augment=False, cache=False, repeat=False)","metadata":{"id":"pZ9HFk1fH0z9","executionInfo":{"status":"ok","timestamp":1655598847747,"user_tz":-330,"elapsed":2,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-07-10T15:57:05.003298Z","iopub.execute_input":"2022-07-10T15:57:05.003862Z","iopub.status.idle":"2022-07-10T15:57:05.679897Z","shell.execute_reply.started":"2022-07-10T15:57:05.003820Z","shell.execute_reply":"2022-07-10T15:57:05.678838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Model Architecture</span>**","metadata":{}},{"cell_type":"markdown","source":"\n![](https://i.imgur.com/eUyPwD0.png)\n\n \nUNETR uses a contracting-expanding pattern consisting of a stack of transformers as the encoder which is connected to a decoder via skip connections. 1D sequence of a 3D input volume x ∈ R^(H×W×D×C) is created with resolution (H,W,D) and C input channels and divide it into flattened uniform non-overlapping patches xv ∈R^(N×(P^3 .C)) where (P, P, P) denotes the resolution of each patch and N = (H×W ×D)/P^3 is the length of the sequence.Then the patches are projected into a K dimensional embedding space using a linear layer and add 1D positional embedding to it. After embeddings a stack of transformer blocks consisting of multi-head self-attention (MSA) and multilayer perceptron (MLP) sublayers are used.\n    \n","metadata":{}},{"cell_type":"code","source":"class SingleDeconv3DBlock(tf.keras.layers.Layer):\n\n    def __init__(self,filters):\n        super(SingleDeconv3DBlock, self).__init__()\n        self.block = tf.keras.layers.Conv3DTranspose(filters= filters, \n                                                     kernel_size=2, strides=2, \n                                                     padding=\"valid\", \n                                                     output_padding=None)\n                                                     \n\n    def call(self, inputs):        \n        return self.block(inputs)\n\n\n\nclass SingleConv3DBlock(tf.keras.layers.Layer):\n\n    def __init__(self, filters,kernel_size):\n        super(SingleConv3DBlock, self).__init__()\n        self.kernel=kernel_size\n        self.res = tuple(map(lambda i: (i - 1)//2, self.kernel))\n        self.block = tf.keras.layers.Conv3D(filters= filters, \n                                            kernel_size=kernel_size, \n                                            strides=1, \n                                            padding='same')\n\n    def call(self, inputs):\n        return self.block(inputs)\n    \nclass Conv3DBlock(tf.keras.layers.Layer):\n\n    def __init__(self, filters,kernel_size=(3,3,3)):\n        super(Conv3DBlock, self).__init__()\n        self.a= tf.keras.Sequential([\n                                     SingleConv3DBlock(filters,kernel_size=kernel_size),\n                                     tf.keras.layers.BatchNormalization(),\n                                     tf.keras.layers.Activation('relu')\n        ])\n        \n\n    def call(self, inputs):\n        return self.a(inputs)\n    \nclass Deconv3DBlock(tf.keras.layers.Layer):\n\n    def __init__(self, filters,kernel_size=(3,3,3)):\n        super(Deconv3DBlock, self).__init__()\n        self.a= tf.keras.Sequential([\n                                     SingleDeconv3DBlock(filters=filters),\n                                     SingleConv3DBlock(filters=filters,kernel_size=kernel_size),\n                                     tf.keras.layers.BatchNormalization(),\n                                     tf.keras.layers.Activation('relu')\n        ])\n  \n    def call(self, inputs):\n        return self.a(inputs)\n    \n","metadata":{"id":"gLuldKPYCNJp","executionInfo":{"status":"ok","timestamp":1655598852877,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-07-10T15:57:05.681593Z","iopub.execute_input":"2022-07-10T15:57:05.682459Z","iopub.status.idle":"2022-07-10T15:57:05.699621Z","shell.execute_reply.started":"2022-07-10T15:57:05.682410Z","shell.execute_reply":"2022-07-10T15:57:05.698841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nMLP comprises two linear layers with GELU activation functions, i is the intermediate block identifier, and L is the number of transformer layers.A MSA sublayer comprises parallel self-attention (SA) heads. The SA block is a parameterized function that learns the mapping between a query (q) and the corresponding key (k) and value (v) representations in a sequence. The attention weights (A) are computed by measuring the similarity between two elements in z and their key-value pairs using softmax function. \n    \n","metadata":{}},{"cell_type":"code","source":"class SelfAttention(tf.keras.layers.Layer):\n\n    def __init__(self, num_heads,embed_dim,dropout):\n        super(SelfAttention, self).__init__()\n\n        self.num_attention_heads = num_heads\n        self.attention_head_size = int(embed_dim / num_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query=tf.keras.layers.Dense(self.all_head_size)\n        self.key = tf.keras.layers.Dense(self.all_head_size)\n        self.value = tf.keras.layers.Dense(self.all_head_size)                \n\n        self.out=tf.keras.layers.Dense(embed_dim)\n        self.attn_dropout=tf.keras.layers.Dropout(dropout)\n        self.proj_dropout=tf.keras.layers.Dropout(dropout)\n\n        self.softmax=tf.keras.layers.Softmax()\n\n        self.vis=False\n\n    def transpose_for_scores(self,x):\n        new_x_shape=list(x.shape[:-1] + (self.num_attention_heads, self.attention_head_size))\n        new_x_shape[0] = -1\n        y = tf.reshape(x, new_x_shape)\n        return tf.transpose(y,perm=[0,2,1,3])\n\n    def call(self, hidden_states):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)  \n        attention_scores= query_layer @ tf.transpose(key_layer,perm=[0,1,3,2])\n        attention_scores= attention_scores/math.sqrt(self.attention_head_size)\n        attention_probs=self.softmax(attention_scores)\n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)\n\n        context_layer= attention_probs @ value_layer\n        context_layer=tf.transpose( context_layer, perm=[0,2,1,3])\n        new_context_layer_shape = list(context_layer.shape[:-2] + (self.all_head_size,))\n        new_context_layer_shape[0]= -1\n        context_layer = tf.reshape(context_layer,new_context_layer_shape)\n        attention_output = self.out(context_layer)\n        attention_output = self.proj_dropout(attention_output)\n        return attention_output, weights        ","metadata":{"id":"R33LarglnuAZ","executionInfo":{"status":"ok","timestamp":1655598852877,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-07-10T15:57:05.702045Z","iopub.execute_input":"2022-07-10T15:57:05.702675Z","iopub.status.idle":"2022-07-10T15:57:05.721030Z","shell.execute_reply.started":"2022-07-10T15:57:05.702636Z","shell.execute_reply":"2022-07-10T15:57:05.720321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mlp(tf.keras.layers.Layer):\n\n    def __init__(self, output_features, drop=0.):\n        super(Mlp, self).__init__()\n        self.a=tf.keras.layers.Dense(units=output_features,activation=tf.nn.gelu)\n        self.b=tf.keras.layers.Dropout(drop)\n\n    def call(self, inputs):\n        x=self.a(inputs)\n        return self.b(x)\n\nclass PositionwiseFeedForward(tf.keras.layers.Layer):\n\n    def __init__(self, d_model=768,d_ff=2048, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.a=tf.keras.layers.Dense(units=d_ff)\n        self.b=tf.keras.layers.Dense(units=d_model)\n        self.c=tf.keras.layers.Dropout(dropout)\n\n    def call(self, inputs):\n        return self.b(self.c(tf.nn.relu(self.a(inputs))))\n\n##embeddings, projection_dim=embed_dim\nclass PatchEmbedding(tf.keras.layers.Layer): \n  def __init__(self ,  cube_size, patch_size , embed_dim):\n        super(PatchEmbedding, self).__init__()\n        self.num_of_patches=int((cube_size[0]*cube_size[1]*cube_size[2])/(patch_size*patch_size*patch_size))\n        self.patch_size=patch_size\n        self.size = patch_size\n        self.embed_dim = embed_dim\n\n        self.projection = tf.keras.layers.Dense(embed_dim)\n\n        self.clsToken = tf.Variable(tf.keras.initializers.GlorotNormal()(shape=(1 , 512 , embed_dim)) , trainable=True)\n\n        self.positionalEmbedding = tf.keras.layers.Embedding(self.num_of_patches , embed_dim)\n        self.patches=None\n        self.lyer = tf.keras.layers.Conv3D(filters= self.embed_dim,kernel_size=self.patch_size, strides=self.patch_size,padding='valid')\n        #embedding - basically is adding numerical embedding to the layer along with an extra dim  \n      \n  def call(self , inputs):\n        patches =self.lyer(inputs)\n        patches = tf.reshape(patches , (tf.shape(inputs)[0] , -1 , self.size * self.size * 3))\n        patches = self.projection(patches)\n        positions = tf.range(0 , self.num_of_patches , 1)[tf.newaxis , ...]\n        positionalEmbedding = self.positionalEmbedding(positions)\n        patches = patches + positionalEmbedding\n\n        return patches, positionalEmbedding","metadata":{"id":"orvzy6sXuxA3","executionInfo":{"status":"ok","timestamp":1655598852878,"user_tz":-330,"elapsed":4,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-07-10T15:57:05.722390Z","iopub.execute_input":"2022-07-10T15:57:05.722852Z","iopub.status.idle":"2022-07-10T15:57:05.741870Z","shell.execute_reply.started":"2022-07-10T15:57:05.722821Z","shell.execute_reply":"2022-07-10T15:57:05.741128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nA sequence representation zi (i ∈ {3,6,9,12}) is extracted with size H×W×D /P^3 ×K, from the transformer and reshaped into a (H/P) × (W/P) × (D/P) ×K tensor. At the bottleneck of the encoder (i.e. output of the transformer's last layer), a deconvolutional layer is applied to the transformed feature map to increase its resolution by a factor of 2. The resized feature map is then concatenated with the feature map of the previous transformer output (e.g. z9), and fed into consecutive 3 × 3 × 3 convolutional layers and the output is upsampled using a deconvolutional layer. This process is repeated for all the other subsequent layers up to the original input resolution where the final output is fed into a 1×1×1 convolutional layer with a softmax activation function to generate voxel-wise semantic predictions.\n    ","metadata":{"id":"mznQvGsEOJK5"}},{"cell_type":"code","source":"##transformerblock\nclass TransformerLayer(tf.keras.layers.Layer):\n    def __init__(self ,  embed_dim, num_heads ,dropout, cube_size, patch_size):\n      super(TransformerLayer,self).__init__()\n\n      self.attention_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n      self.mlp_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n#embed_dim/no-of_heads\n      self.mlp_dim = int((cube_size[0] * cube_size[1] * cube_size[2]) / (patch_size * patch_size * patch_size))\n      \n      self.mlp = PositionwiseFeedForward(embed_dim,2048)\n      self.attn = SelfAttention(num_heads, embed_dim, dropout)\n    \n    def call(self ,x  , training=True):\n      h=x\n      x=self.attention_norm(x)\n      x,weights= self.attn(x)\n      x=x+h\n      h=x\n\n      x = self.mlp_norm(x)\n      x = self.mlp(x)\n\n      x = x + h\n\n      return x, weights\n\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n  def __init__(self ,embed_dim , num_heads,cube_size, patch_size , num_layers=12 , dropout=0.1,extract_layers=[3,6,9,12]):\n    super(TransformerEncoder,self).__init__()\n#  embed_dim, num_heads ,dropout, cube_size, patch_size\n    self.embeddings = PatchEmbedding(cube_size,patch_size, embed_dim)\n    self.extract_layers =extract_layers\n    self.encoders = [TransformerLayer(embed_dim, num_heads,dropout, cube_size, patch_size) for _ in range(num_layers)]\n  \n  def call(self , inputs , training=True):\n    extract_layers = []\n    x = inputs\n    x,_=self.embeddings(x)\n    \n    for depth,layer in enumerate(self.encoders):\n      x,_= layer(x , training=training)\n      if depth + 1 in self.extract_layers:\n                extract_layers.append(x)\n    \n    return extract_layers","metadata":{"id":"Wyv90IvrTM7r","executionInfo":{"status":"ok","timestamp":1655598853319,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-07-10T15:57:05.744038Z","iopub.execute_input":"2022-07-10T15:57:05.745015Z","iopub.status.idle":"2022-07-10T15:57:05.761041Z","shell.execute_reply.started":"2022-07-10T15:57:05.744966Z","shell.execute_reply":"2022-07-10T15:57:05.760384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNETR(tf.keras.Model):\n    def __init__(self, img_shape=(96,96, 96), input_dim=3, output_dim=3, embed_dim=768, patch_size=16, num_heads=12, dropout=0.1):\n        super(UNETR,self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.embed_dim = embed_dim\n        self.img_shape = img_shape\n        self.patch_size = patch_size\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.num_layers = 12\n        self.ext_layers = [3, 6, 9, 12]\n        \n        self.patch_dim = [int(x / patch_size) for x in img_shape]\n        self.transformer = \\\n            TransformerEncoder(\n                self.embed_dim,\n                self.num_heads,\n                self.img_shape,\n                self.patch_size,\n                self.num_layers,\n                self.dropout,\n                self.ext_layers\n            )\n        \n        # U-Net Decoder\n        self.decoder0 = \\\n            tf.keras.Sequential([\n                Conv3DBlock(32, (3,3,3)),\n                Conv3DBlock(64, (3,3,3))]\n            )\n      \n        self.decoder3 = \\\n            tf.keras.Sequential([\n                Deconv3DBlock(512),\n                Deconv3DBlock(256),\n                Deconv3DBlock(128)]\n            )\n   \n        self.decoder6 = \\\n            tf.keras.Sequential([\n                Deconv3DBlock(512),\n                Deconv3DBlock(256)]\n            )\n    \n        self.decoder9 = \\\n            Deconv3DBlock(512)\n\n        self.decoder12_upsampler = \\\n            SingleDeconv3DBlock(512)\n\n        self.decoder9_upsampler = \\\n            tf.keras.Sequential([\n                Conv3DBlock(512),\n                Conv3DBlock(512),\n                Conv3DBlock(512),\n                SingleDeconv3DBlock(256)]\n            )\n\n        self.decoder6_upsampler = \\\n            tf.keras.Sequential([\n                Conv3DBlock(256),\n                Conv3DBlock(256),\n                SingleDeconv3DBlock(128)]\n            )\n\n        self.decoder3_upsampler = \\\n            tf.keras.Sequential(\n                [Conv3DBlock(128),\n                Conv3DBlock(128),\n                SingleDeconv3DBlock(64)]\n            )\n\n        self.decoder0_header = \\\n            tf.keras.Sequential(\n                [Conv3DBlock(64),\n                Conv3DBlock(64),\n                SingleConv3DBlock(output_dim, (1,1,1))]\n            ) \n\n \n    def call(self, x):\n        z = self.transformer(x)\n        z0, z3, z6, z9, z12 = x, z[0],z[1],z[2],z[3]\n        z3 = tf.reshape(tf.transpose(z3,perm=[0,2,1]),[-1,  *self.patch_dim,self.embed_dim])\n        z6 = tf.reshape(tf.transpose(z6,perm=[0,2,1]),[-1,  *self.patch_dim,self.embed_dim])\n        z9 = tf.reshape(tf.transpose(z9,perm=[0,2,1]),[-1,  *self.patch_dim,self.embed_dim])\n        z12 = tf.reshape(tf.transpose(z12,perm=[0,2,1]),[-1,  *self.patch_dim,self.embed_dim])\n        z12 = self.decoder12_upsampler(z12)\n        z9 = self.decoder9(z9)\n        z9 = self.decoder9_upsampler(tf.concat([z9, z12], 4))\n        z6 = self.decoder6(z6)\n        z6 = self.decoder6_upsampler(tf.concat([z6, z9], 4))\n        z3 = self.decoder3(z3)\n        z3 = self.decoder3_upsampler(tf.concat([z3, z6], 4))\n        z0 = self.decoder0(z0)\n        output = self.decoder0_header(tf.concat([z0, z3], 4))\n        return output\n        ","metadata":{"id":"mvULdDmYdy_I","executionInfo":{"status":"ok","timestamp":1655598853319,"user_tz":-330,"elapsed":3,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"execution":{"iopub.status.busy":"2022-07-10T15:57:05.762136Z","iopub.execute_input":"2022-07-10T15:57:05.762649Z","iopub.status.idle":"2022-07-10T15:57:05.786620Z","shell.execute_reply.started":"2022-07-10T15:57:05.762613Z","shell.execute_reply":"2022-07-10T15:57:05.785698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Loss Functions</span>**","metadata":{}},{"cell_type":"code","source":"from segmentation_models.base import functional as F\nimport tensorflow.keras.backend as K\n\nkwargs = {}\nkwargs[\"backend\"] = K  # set tensorflow.keras as backend\n\n\ndef dice_coef(y_true, y_pred):\n    \"\"\"Dice coefficient\"\"\"\n    dice = F.f_score(\n        y_true,\n        y_pred,\n        beta=1,\n        smooth=1e-5,\n        per_image=False,\n        threshold=0.5,\n        **kwargs,\n    )\n    return dice\n\ndef tversky(y_true, y_pred, axis=(0, 1, 2), alpha=0.3, beta=0.7, smooth=0.0001):\n    \"Tversky metric\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    tp = tf.math.reduce_sum(y_true * y_pred, axis=axis) # calculate True Positive\n    fn = tf.math.reduce_sum(y_true * (1 - y_pred), axis=axis) # calculate False Negative\n    fp = tf.math.reduce_sum((1 - y_true) * y_pred, axis=axis) # calculate False Positive\n    tv = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth) # calculate tversky\n    tv = tf.math.reduce_mean(tv)\n    return tv\n\n\ndef tversky_loss(y_true, y_pred):\n    \"Tversky Loss\"\n    return 1 - tversky(y_true, y_pred)\n\n\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    \"Focal Tversky Loss: Focal Loss + Tversky Loss\"\n    tv = tversky(y_true, y_pred)\n    return K.pow((1 - tv), gamma)\n\n\n# Register custom objects\ncustom_objs = {\n    \"dice_loss\": sm.losses.dice_loss,\n    \"dice_coef\": dice_coef,\n    \"bce_dice_loss\": sm.losses.bce_dice_loss,\n    \"bce_jaccard_loss\": sm.losses.bce_jaccard_loss,\n    \"tversky_loss\": tversky_loss,\n    \"focal_tversky_loss\": focal_tversky_loss,\n    \"jaccard_loss\": sm.losses.jaccard_loss,\n    \"precision\": sm.metrics.precision,\n    \"recall\": sm.metrics.recall,\n}\ntf.keras.utils.get_custom_objects().update(custom_objs)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-10T15:57:05.787742Z","iopub.execute_input":"2022-07-10T15:57:05.788511Z","iopub.status.idle":"2022-07-10T15:57:05.805216Z","shell.execute_reply.started":"2022-07-10T15:57:05.788463Z","shell.execute_reply":"2022-07-10T15:57:05.804224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Callbacks - LR schedule</span>**","metadata":{}},{"cell_type":"code","source":"def get_lr_callback():\n    if CFG.lr_schedule == \"ReduceLROnPlateau\":\n        lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.1,\n            patience=int(CFG.patience / 2),\n            min_lr=CFG.lr / 1e2,\n        )\n    elif CFG.lr_schedule == \"CosineDecay\":\n        lr_schedule = tf.keras.experimental.CosineDecay(\n            initial_learning_rate=CFG.lr, decay_steps=CFG.epochs + 2, alpha=CFG.lr / 1e2\n        )\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n    elif CFG.lr_schedule == \"ExponentialDecay\":\n        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n            initial_learning_rate=CFG.lr,\n            decay_steps=CFG.epochs + 2,\n            decay_rate=0.05,\n            staircase=False,\n        )\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n    return lr_schedule","metadata":{"execution":{"iopub.status.busy":"2022-07-10T15:57:05.806649Z","iopub.execute_input":"2022-07-10T15:57:05.806878Z","iopub.status.idle":"2022-07-10T15:57:05.821881Z","shell.execute_reply.started":"2022-07-10T15:57:05.806854Z","shell.execute_reply":"2022-07-10T15:57:05.820984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Model Summary</span>**","metadata":{}},{"cell_type":"code","source":"def get_model(name=CFG.model_name, loss=CFG.loss, backbone=CFG.backbone):\n    #model = TransUNet(image_size=CFG.img_size[0], freeze_enc_cnn=False, pretrain=True)\n    model = UNETR()\n    lr = CFG.lr\n    if CFG.optimizer == \"Adam\":\n        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n    elif CFG.optimizer == \"AdamW\":\n        opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=lr)\n    elif CFG.optimizer == \"RectifiedAdam\":\n        opt = tfa.optimizers.RectifiedAdam(learning_rate=lr)\n    else:\n        raise ValueError(\"Wrong Optimzer Name\")\n\n    model.compile(\n        optimizer=opt,\n        loss=loss,\n        steps_per_execution=CFG.steps_per_execution, # to reduce idle time\n        metrics=[\n            dice_coef,\n            \"precision\",\n            \"recall\",\n        ],\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-10T16:00:13.002171Z","iopub.execute_input":"2022-07-10T16:00:13.002606Z","iopub.status.idle":"2022-07-10T16:00:13.010637Z","shell.execute_reply.started":"2022-07-10T16:00:13.002573Z","shell.execute_reply":"2022-07-10T16:00:13.009563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = tf.keras.models.load_model('../input/unetr-model/unetr')\nmodel = get_model()\n#model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-10T16:00:15.831609Z","iopub.execute_input":"2022-07-10T16:00:15.831951Z","iopub.status.idle":"2022-07-10T16:00:16.177237Z","shell.execute_reply.started":"2022-07-10T16:00:15.831916Z","shell.execute_reply":"2022-07-10T16:00:16.176145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nM = {}\n# Which Metrics to store\nmetrics = [\n    \"loss\",\n    \"dice_coef\",\n    \"precision\",\n    \"recall\",\n]\n# Intialize Metrics\nfor fm in metrics:\n    M[\"val_\" + fm] = []\n\nALL_FILENAMES = sorted(ALL_FILENAMES)\n\n# Split tfrecord using KFold\nkf = KFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed) # kfold between trrecord files\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(ALL_FILENAMES)):\n    # If fold is not in selected folds then avoid that fold\n    if fold not in CFG.selected_folds:\n        continue\n        \n    # Train and validation files\n    TRAIN_FILENAMES = [ALL_FILENAMES[i] for i in train_idx]\n    VALID_FILENAMES = [ALL_FILENAMES[i] for i in valid_idx]\n    \n    # Take Only 10 Files if run in Debug Mode\n    if CFG.debug:\n        TRAIN_FILENAMES = TRAIN_FILENAMES[:10]\n        VALID_FILENAMES = VALID_FILENAMES[:10]\n\n    # Shuffle train files\n    random.shuffle(TRAIN_FILENAMES)\n\n    # Count train and valid samples\n    NUM_TRAIN = count_data_items(TRAIN_FILENAMES)\n    NUM_VALID = count_data_items(VALID_FILENAMES)\n\n    # Compute batch size & steps_per_epoch\n    BATCH_SIZE = CFG.batch_size * REPLICAS\n    STEPS_PER_EPOCH = NUM_TRAIN // BATCH_SIZE\n\n    print(\"#\" * 65)\n    print(\"#### FOLD:\", fold)\n    print(\n        \"#### IMAGE_SIZE: (%i, %i) | BATCH_SIZE: %i | EPOCHS: %i\"\n        % (CFG.img_size[0], CFG.img_size[1], BATCH_SIZE, CFG.epochs)\n    )\n    print(\n        \"#### MODEL: %s | BACKBONE: %s | LOSS: %s\"\n        % (CFG.model_name, CFG.backbone, CFG.loss)\n    )\n    print(\"#### NUM_TRAIN: {:,} | NUM_VALID: {:,}\".format(NUM_TRAIN, NUM_VALID))\n    print(\"#\" * 65)\n\n   \n    # Build model in device\n    K.clear_session()\n    with strategy.scope():\n        model = get_model(name=CFG.model_name, backbone=CFG.backbone, loss=CFG.loss)\n\n    # Callbacks\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        \"/kaggle/working/fold-%i.h5\" % fold,\n        verbose=CFG.verbose,\n        monitor=\"val_dice_coef\",\n        mode=\"max\",\n        save_best_only=True,\n        save_weights_only=True,\n    )\n    callbacks = [checkpoint, get_lr_callback()]\n\n    # Create train & valid dataset\n    train_ds = get_dataset(\n        TRAIN_FILENAMES,\n        augment=CFG.augment,\n        batch_size=BATCH_SIZE,\n        cache=False,\n        drop_remainder=False,\n    )\n    valid_ds = get_dataset(\n        VALID_FILENAMES,\n        shuffle=False,\n        augment=False,\n        repeat=False,\n        batch_size=BATCH_SIZE,\n        cache=False,\n        drop_remainder=False,\n    )\n\n    # Train model\n    history = model.fit(\n        train_ds,\n        epochs=CFG.epochs if not CFG.debug else 2,\n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=callbacks,\n        validation_data=valid_ds,\n        #         validation_steps = NUM_VALID/BATCH_SIZE,\n        verbose=CFG.verbose,\n    )\n\n    # Convert dict history to df history\n    history = pd.DataFrame(history.history)\n\n    # Load best weights\n    model.load_weights(\"/kaggle/working/fold-%i.h5\" % fold)\n\n    # Compute & save best valid result\n    print(\"\\nValid Result:\")\n    m = model.evaluate(\n        get_dataset(\n            VALID_FILENAMES,\n            batch_size=BATCH_SIZE,\n            augment=False,\n            shuffle=False,\n            repeat=False,\n            cache=False,\n        ),\n        return_dict=True,\n#        steps=NUM_VALID/BATCH_SIZE,\n        verbose=1,\n    )\n    print()\n    \n    # Store valid results\n    for fm in metrics:\n        M[\"val_\" + fm].append(m[fm])\n        \n \n    # Plot Training History\n    if CFG.display_plot:\n        plt.figure(figsize=(15, 5))\n        plt.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"dice_coef\"],\n            \"-o\",\n            label=\"Train Dice\",\n            color=\"#ff7f0e\",\n        )\n        plt.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"val_dice_coef\"],\n            \"-o\",\n            label=\"Val Dice\",\n            color=\"#1f77b4\",\n        )\n        x = np.argmax(history[\"val_dice_coef\"])\n        y = np.max(history[\"val_dice_coef\"])\n        xdist = plt.xlim()[1] - plt.xlim()[0]\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x, y, s=200, color=\"#1f77b4\")\n        plt.text(x - 0.03 * xdist, y - 0.13 * ydist, \"max dice\\n%.2f\" % y, size=14)\n        plt.ylabel(\"dice_coef\", size=14)\n        plt.xlabel(\"Epoch\", size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"loss\"],\n            \"-o\",\n            label=\"Train Loss\",\n            color=\"#2ca02c\",\n        )\n        plt2.plot(\n            np.arange(len(history[\"dice_coef\"])),\n            history[\"val_loss\"],\n            \"-o\",\n            label=\"Val Loss\",\n            color=\"#d62728\",\n        )\n        x = np.argmin(history[\"val_loss\"])\n        y = np.min(history[\"val_loss\"])\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x, y, s=200, color=\"#d62728\")\n        plt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min loss\", size=14)\n        plt.ylabel(\"Loss\", size=14)\n        plt.title(\"FOLD %i\" % (fold), size=18)\n        plt.legend(loc=3)\n        plt.savefig(f\"fig-{fold}.png\")\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-10T16:00:18.581569Z","iopub.execute_input":"2022-07-10T16:00:18.581977Z","iopub.status.idle":"2022-07-10T16:31:32.406395Z","shell.execute_reply.started":"2022-07-10T16:00:18.581938Z","shell.execute_reply":"2022-07-10T16:31:32.405285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Making Predictions</span>**","metadata":{}},{"cell_type":"code","source":"#pred = model.predict(ds.skip(200).take(1))\n","metadata":{"id":"dRwY76d6k3aK","executionInfo":{"status":"ok","timestamp":1655600158095,"user_tz":-330,"elapsed":15744,"user":{"displayName":"Tensor Girl","userId":"02972928350054045429"}},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-07-10T15:59:23.794956Z","iopub.status.idle":"2022-07-10T15:59:23.795396Z","shell.execute_reply.started":"2022-07-10T15:59:23.795154Z","shell.execute_reply":"2022-07-10T15:59:23.795178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## **<span style=\"color:#F7B2B0;\">References</span>**\n\nhttps://www.kaggle.com/code/usharengaraju/tensorflow-unetr-w-b\n\nhttps://arxiv.org/pdf/2103.10504.pdf\n\nhttps://github.com/tamasino52/UNETR (Pytorch)\n\nhttps://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/unetr_btcv_segmentation_3d_lightning.ipynb (Pytorch)\n\nhttps://www.kaggle.com/code/awsaf49/uwmgi-transunet-2-5d-train-tf\n\nhttps://www.kaggle.com/datasets/awsaf49/uwmgi-25d-tfrecord-dataset\n\nhttps://www.kaggle.com/code/bsridatta/eda-for-a-healthy-gi-tract\n\nhttps://www.kaggle.com/datasets/bsridatta/uwmadison-flattened-metadata\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}